{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HEGolzhYDpMY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "!pip install contractions\n",
    "!pip install inflect\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ecgzA6gwExdE"
   },
   "outputs": [],
   "source": [
    "def DoctoDict():\n",
    "  file_Names = os.listdir(\"/content/drive/MyDrive/IR_ASSIGNMENT_1/stories1\")\n",
    "  # print(file_lists)\n",
    "  file_Paths = []\n",
    "  for i in range(len(file_Names)):\n",
    "    file_Paths.append(\"/content/drive/MyDrive/IR_ASSIGNMENT_1/stories1/\"+file_Names[i])\n",
    "  corpus = {}\n",
    "  for i in range(len(file_Paths)):\n",
    "    with open(file_Paths[i],encoding = \"latin-1\") as f_input:\n",
    "      corpus[file_Names[i]] = [f_input.read()]\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "DVYdVrsbI7JJ"
   },
   "outputs": [],
   "source": [
    "# CORPUS = DoctoDict()\n",
    "# with open('CORPUS.pickle', 'wb') as handle:\n",
    "    # pickle.dump(CORPUS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('/content/CORPUS.pickle', 'rb') as handle:\n",
    "    corpus = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "eM6mJ8drTPNA"
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    text=re.sub('\\n',' ',text)\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word,pos=\"v\")\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def preProcess_html(fileName):\n",
    "    sample = denoise_text(fileName)\n",
    "    sample = replace_contractions(sample)\n",
    "    words = nltk.word_tokenize(sample)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize(words)\n",
    "    return words  \n",
    "\n",
    "def clean_text(text):\n",
    "    # text=re.sub('\\w*\\d\\w*','', text)\n",
    "    text=re.sub('\\n',' ',text)\n",
    "    text=re.sub(r\"http\\S+\", \"\", text)\n",
    "    text=re.sub('[^a-z0-9A-Z]',' ',text)\n",
    "    text=re.sub(' +',' ',text)\n",
    "    return text\n",
    "\n",
    "def preProcessotherfiles(fileName):\n",
    "    sample = clean_text(fileName)\n",
    "    sample = replace_contractions(sample)\n",
    "    words = nltk.word_tokenize(sample)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize(words)\n",
    "    return words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "eFsKQI7MlBg7"
   },
   "outputs": [],
   "source": [
    "for i in corpus.keys():\n",
    "  if i.endswith(\".html\") or i.endswith(\".header\"):\n",
    "    corpus[i][0] = preProcess_html(corpus[i][0])\n",
    "  else:\n",
    "    corpus[i][0] = preProcessotherfiles(corpus[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lt88SyFbmeJm",
    "outputId": "f160d566-1fc5-4f89-8f4e-cf29c17bc973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([' (2).descs', '5orange.txt', '3wishes.txt', '4moons.txt', '3student.txt', '.header', '3lpigs.txt', ' (2).header', '3gables.txt', '.windex.html', '3sonnets.vrs', '.descs', '.musings', '.footer', '19.lws', '18.lws', '17.lws', '16.lws', '6napolen.txt', '7voysinb.txt', '7oldsamr.txt', '13chil.txt', '6ablemen.txt', '14.lws', 'aircon.txt', 'aesop11.txt', 'advsayed.txt', 'advtthum.txt', 'adv_alad.txt', 'abbey.txt', 'abyss.txt', 'adler.txt', 'ab40thv.txt', '100west.txt', '20.lws', 'bookem.1', 'bookem2', 'bookem3', 'bluebrd.txt', 'blind.txt', 'blh.txt', 'blasters.fic', 'blossom.pom', 'blue', 'blak', 'blabnove.txt', 'blackp.txt', 'bishop00.txt', 'bgb.txt', 'bgcspoof.txt', 'blackrdr', 'blabnove.hum', 'bigred.hum', 'beyond.hum', 'berternie.txt', 'beggars.txt', 'beast.asc', 'bestwish', 'beautbst.txt', 'assorted.txt', 'bagelman.txt', 'batlslau.txt', 'bern', 'bagel.man', 'asop', 'arctic.txt', 'aquith.txt', 'antcrick.txt', 'angry_ca.txt', 'aminegg.txt', 'archive', 'arcadia.sty', 'aluminum.hum', 'angelfur.hum', 'alad10.txt', 'aesopa10.txt', 'aislesix.txt', 'alissadl.txt', 'altside.hum', 'aisle.six', 'consumdr.hum', 'confilct.fun', 'contrad1.hum', 'cooldark.sto', 'comp', 'cmoutmou.txt', 'clevdonk.txt', 'charlie.txt', 'ccm.txt', 'cardcnt.txt', 'campfire.txt', 'chik', 'clon', 'candle.hum', 'cameloto.hum', 'cabin.txt', 'burltrs', 'burintrv.78', 'burintrv.92', 'burn', 'bureau.txt', 'bulzork1.txt', 'burintrv.66', 'bulolli2.txt', 'bulphrek.txt', 'bulolli1.txt', 'bulnoopt.txt', 'bulnland.txt', 'bumm', 'bulwer.lytton', 'bulprint.txt', 'bulmrx.txt', 'bulironb.txt', 'bulhuntr.txt', 'bullove.txt', 'buldetal.txt', 'buggy.txt', 'buldream.txt', 'breaks3.asc', 'bruce-p.txt', 'bulfelis.txt', 'breaks2.asc', 'breaks1.asc', 'bran', 'bram', 'brain.damage', 'fantasy.txt', 'fable.txt', 'excerpt.txt', 'eyeargon.hum', 'ezoff', 'fantasy.hum', 'excerpt.hum', 'fantas.hum', 'enya_trn.txt', 'enginer.txt', 'encamp01.txt', 'empsjowk.txt', 'empty.txt', 'emperor3.txt', 'empnclot.txt', 'enchdup.hum', 'enc', 'elveshoe.txt', 'dwar', 'elite.app', 'dtruck.txt', 'dopedenn.txt', 'dskool.txt', 'discocanbefun.txt', 'dicksong.txt', 'domain.poe', 'dicegame.txt', 'diaryflf.txt', 'disco.be.fun', 'descent.poe', 'deer.txt', 'darkness.txt', 'dakota.txt', 'deathmrs.d', 'day.in.mcdonald', 'deal', 'dan', 'cybersla.txt', 'curious.george', 'cum', 'crazy.hum', 'crabhern.txt', 'cooldark.txt', 'cow.exploder', 'corcor.hum', 'goldenp.txt', 'gold3ber.txt', 'goldbug.poe', 'gloves.txt', 'glimpse1.txt', 'girlclub.txt', 'gemdra.txt', 'game.txt', 'gatherng.txt', 'frogp.txt', 'girl', 'ghost', 'gay', 'frum', 'friends.txt', 'fred.txt', 'foxncrow.txt', 'foxngrap.txt', 'foxnstrk.txt', 'freeman.fil', 'friend.s', 'fowl.death', 'fran', 'fourth.fic', 'floobs.txt', 'fish.txt', 'fleas.txt', 'flktrp.txt', 'forgotte', 'flute.txt', 'flytrunk.txt', 'floc', 'fic7', 'fic5', 'fic4', 'fic2', 'fic3', 'fic1', 'fgoose.txt', 'fea3', 'fea2', 'fea1', 'fearmnky', 'fear.hum', 'jim.asc', 'jaynejob.asc', 'jackbstl.txt', 'index.html', 'index (3).html', 'index (2).html', 'jerichms.hum', 'jackmac.fic', 'island.poe', 'inter', 'imonly17.txt', 'hound-b.txt', 'hotline4.txt', 'hotline3.txt', 'imagin.hum', 'immorti.hum', 'immortal', 'idi.hum', 'igiv', 'hitch2.txt', 'horswolf.txt', 'hitch3.txt', 'horsdonk.txt', 'holmesbk.txt', 'how.ernie.bert', 'hotline1.txt', 'hareporc.txt', 'haretort.txt', 'hell4.txt', 'helmfuse.txt', 'hareleph.txt', 'hellmach.txt', 'healer.txt', 'home.fil', 'hop-frog.poe', 'hole2nar.txt', 'history5.txt', 'hils', 'hansgrtl.txt', 'gulliver.txt', 'greedog.txt', 'goldgoos.txt', 'graymare.txt', 'goldfish.txt', 'grav', 'greatlrn.leg', 'nigel.1', 'mydream.txt', 'nigel.4', 'myeyes', 'narciss.txt', 'musibrem.txt', 'nigel.3', 'nigel.2', 'mtinder.txt', 'mouslion.txt', 'monksol.txt', 'monkking.txt', 'modemhippy.txt', 'musgrave.txt', 'nigel.5', 'missing.txt', 'mindprob.txt', 'mike.txt', 'mazarin.txt', 'mattress.txt', 'lure.txt', 'mcdonaldl.txt', 'mario.txt', 'mindwar', 'melissa.txt', 'luf', 'lrrhood.txt', 'lpeargrl.txt', 'ltp', 'long1-3.txt', 'lmtchgrl.txt', 'lmermaid.txt', 'lionwar.txt', 'lionmosq.txt', 'life.txt', 'lionmane.txt', 'lgoldbrd.txt', 'lionbird', 'lil', 'lament.txt', 'kneeslapper.txt', 'kzap.txt', 'knuckle.txt', 'kharian.txt', 'keepmodu.txt', 'ladylust.hum', 'kneeslapper', 'korea.s', 'keeping.insanit', 'quarter.c8', 'quarter.c4', 'pussboot.txt', 'quarter.c3', 'quarter.c2', 'quarter.c1', 'qcarroll', 'psf.txt', 'pphamlin.txt', 'pinocch.txt', 'poplstrm.txt', 'poem-1.txt', 'plescopm.txt', 'poem-2.txt', 'poem-4.txt', 'psyc', 'progx', 'psi', 'prince.art', 'pregn.txt', 'piracy.sto', 'parotsha.txt', 'paink-ws.txt', 'pepdegener.txt', 'panama.txt', 'partya.txt', 'perf', 'peace.fun', 'pepsi.degenerat', 'paul_har.sto', 'oxfrog.txt', 'outcast.dos', 'obstgoat.txt', 'omarsheh.txt', 'nigel.10', 'non2', 'nihgel_8.9', 'non3', 'nigel.7', 'nitepeek.sto', 'non4', 'nigel.6', 'spider.txt', 'spectacl.poe', 'spam.key', 'social.vikings', 'socialvikings.txt', 'solitary.txt', 'snowqn1.txt', 'snow.txt', 'snowmaid.txt', 'space.txt', 'sleprncs.txt', 'sick-kid.txt', 'shrdfarm.txt', 'sight.txt', 'shulk.txt', 'silverb.txt', 'sis', 'shoscomb.txt', 'sanpedr2.txt', 'safe', 's&m_plot', 's&m_that', 'running.txt', 'roger1.txt', 'retrib.txt', 'rid.txt', 'reality.txt', 'rocket.sf', 'rock', 'robotech', 'reap', 'redragon.txt', 'rainda.txt', 'radar_ra.txt', 'quot', 'quickfix', 'quarter.c15', 'quarter.c16', 'quarter.c19', 'quarter.c18', 'quest', 'quarter.c17', 'quarter.c14', 'quarter.c13', 'quarter.c6', 'quarter.c9', 'quarter.c7', 'quarter.c5', 'quarter.c12', 'quarter.c10', 'quarter.c11', 'times.fic', 'thewave', 'timem.hac', 'the-tree.txt', 'telefone.txt', 'tcoa.txt', 'tearglas.txt', 'tctac.txt', 'taxnovel.txt', 'thanksg', 'terrorbears.txt', 'testpilo.hum', 'textfile.primer', 'tao3.dos', 'tailbear.txt', 'sunday.txt', 'sucker.txt', 't_zone.jok', 'startrek.txt', 'stairdre.txt', 'szechuan', 'superg1', 'stsgreek', 'sre-dark.txt', 'sre08.txt', 'sre09.txt', 'sretrade.txt', 'sre10.txt', 'stainles.ana', 'srex.txt', 'sre07.txt', 'sre06.txt', 'sre05.txt', 'sre04.txt', 'sre_sei.txt', 'sre01.txt', 'sre02.txt', 'sre_finl.txt', 'sre03.txt', 'sre_feqh.txt', 'spiders.txt', 'sqzply.txt', 'zombies.txt', 'yukon.txt', 'wolflamb.txt', 'write', 'wrt', 'wombat.und', 'wolfcran.txt', 'weeprncs.txt', 'wisteria.txt', 'wlgirl.txt', 'weaver.txt', 'wolf7kid.txt', 'whgdsreg.reg', 'wanderer.fun', 'withdraw.cyb', 'wall.art', 'vgilante.txt', 'vainsong.txt', 'vaincrow.txt', 'unluckwr.txt', 'uglyduck.txt', 'veiledl.txt', 'vampword.txt', 'vday.hum', 'valen', 'tuc_mees', 'tree.txt', 'timetrav.txt', 'tinsoldr.txt', 'traitor.txt', 'tin', 'toilet.s'])"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1idUem9nAVF",
    "outputId": "cdc62414-c442-4c66-9c5d-7a96a1d42892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'mouse',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'upon',\n",
       " 'time',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'trip',\n",
       " 'country',\n",
       " 'meet',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'spend',\n",
       " 'day',\n",
       " 'together',\n",
       " 'become',\n",
       " 'friends',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'take',\n",
       " 'new',\n",
       " 'friend',\n",
       " 'meadows',\n",
       " 'vegetable',\n",
       " 'garden',\n",
       " 'make',\n",
       " 'sample',\n",
       " 'good',\n",
       " 'things',\n",
       " 'land',\n",
       " 'never',\n",
       " 'see',\n",
       " 'beauties',\n",
       " 'countryside',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'thrill',\n",
       " 'though',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'plain',\n",
       " 'food',\n",
       " 'nearly',\n",
       " 'fine',\n",
       " 'usual',\n",
       " 'meals',\n",
       " 'thank',\n",
       " 'friend',\n",
       " 'lovely',\n",
       " 'out',\n",
       " 'invite',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'visit',\n",
       " 'town',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'saw',\n",
       " 'pantry',\n",
       " 'friend',\n",
       " 'house',\n",
       " 'full',\n",
       " 'ham',\n",
       " 'cheese',\n",
       " 'oil',\n",
       " 'flour',\n",
       " 'honey',\n",
       " 'jam',\n",
       " 'stack',\n",
       " 'goodies',\n",
       " 'stand',\n",
       " 'speechless',\n",
       " 'surprise',\n",
       " 'never',\n",
       " 'see',\n",
       " 'anything',\n",
       " 'like',\n",
       " 'wonderful',\n",
       " 'things',\n",
       " 'eat',\n",
       " 'course',\n",
       " 'come',\n",
       " 'ply',\n",
       " 'guest',\n",
       " 'tuck',\n",
       " 'begin',\n",
       " 'feast',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'try',\n",
       " 'stuff',\n",
       " 'want',\n",
       " 'taste',\n",
       " 'everything',\n",
       " 'find',\n",
       " 'tummy',\n",
       " 'full',\n",
       " 'luckiest',\n",
       " 'mouse',\n",
       " 'ever',\n",
       " 'meet',\n",
       " 'say',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'town',\n",
       " 'brother',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'listen',\n",
       " 'delight',\n",
       " 'friend',\n",
       " 'praise',\n",
       " 'suddenly',\n",
       " 'sound',\n",
       " 'heavy',\n",
       " 'footsteps',\n",
       " 'interrupt',\n",
       " 'feast',\n",
       " 'run',\n",
       " 'whisper',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'friend',\n",
       " 'time',\n",
       " 'within',\n",
       " 'inch',\n",
       " 'stand',\n",
       " 'lady',\n",
       " 'house',\n",
       " 'large',\n",
       " 'foot',\n",
       " 'luckily',\n",
       " 'lady',\n",
       " 'go',\n",
       " 'away',\n",
       " 'two',\n",
       " 'mice',\n",
       " 'return',\n",
       " 'enjoy',\n",
       " 'meal',\n",
       " 'rudely',\n",
       " 'interrupt',\n",
       " 'right',\n",
       " 'come',\n",
       " 'say',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'worry',\n",
       " 'go',\n",
       " 'honey',\n",
       " 'delicious',\n",
       " 'ever',\n",
       " 'taste',\n",
       " 'yes',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'lie',\n",
       " 'try',\n",
       " 'sound',\n",
       " 'casual',\n",
       " 'taste',\n",
       " 'exclaim',\n",
       " 'scrumptious',\n",
       " 'king',\n",
       " 'mice',\n",
       " 'never',\n",
       " 'eat',\n",
       " 'anything',\n",
       " 'lovely',\n",
       " 'life',\n",
       " 'suddenly',\n",
       " 'come',\n",
       " 'sound',\n",
       " 'footsteps',\n",
       " 'time',\n",
       " 'thump',\n",
       " 'heavily',\n",
       " 'two',\n",
       " 'mice',\n",
       " 'flee',\n",
       " 'man',\n",
       " 'house',\n",
       " 'come',\n",
       " 'fetch',\n",
       " 'bottle',\n",
       " 'saw',\n",
       " 'spill',\n",
       " 'honey',\n",
       " 'groan',\n",
       " 'ghastly',\n",
       " 'mice',\n",
       " 'think',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'send',\n",
       " 'cat',\n",
       " 'tremble',\n",
       " 'terror',\n",
       " 'mice',\n",
       " 'hide',\n",
       " 'away',\n",
       " 'time',\n",
       " 'sudden',\n",
       " 'visit',\n",
       " 'give',\n",
       " 'fright',\n",
       " 'man',\n",
       " 'awful',\n",
       " 'word',\n",
       " 'mice',\n",
       " 'scar',\n",
       " 'hold',\n",
       " 'breath',\n",
       " 'make',\n",
       " 'sound',\n",
       " 'since',\n",
       " 'remain',\n",
       " 'quiet',\n",
       " 'begin',\n",
       " 'feel',\n",
       " 'braver',\n",
       " 'pick',\n",
       " 'enough',\n",
       " 'courage',\n",
       " 'leave',\n",
       " 'hidey',\n",
       " 'hole',\n",
       " 'come',\n",
       " 'nobody',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'whisper',\n",
       " 'suddenly',\n",
       " 'pantry',\n",
       " 'door',\n",
       " 'creak',\n",
       " 'two',\n",
       " 'luckless',\n",
       " 'mice',\n",
       " 'freeze',\n",
       " 'fear',\n",
       " 'dim',\n",
       " 'light',\n",
       " 'glow',\n",
       " 'pair',\n",
       " 'horrid',\n",
       " 'yellow',\n",
       " 'eye',\n",
       " 'large',\n",
       " 'cat',\n",
       " 'star',\n",
       " 'round',\n",
       " 'room',\n",
       " 'search',\n",
       " 'prey',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'town',\n",
       " 'mouse',\n",
       " 'tiptoe',\n",
       " 'silently',\n",
       " 'back',\n",
       " 'hidey',\n",
       " 'hole',\n",
       " 'wish',\n",
       " 'pound',\n",
       " 'hearts',\n",
       " 'would',\n",
       " 'stop',\n",
       " 'beat',\n",
       " 'fear',\n",
       " 'cat',\n",
       " 'hear',\n",
       " 'noise',\n",
       " 'make',\n",
       " 'luck',\n",
       " 'would',\n",
       " 'cat',\n",
       " 'discover',\n",
       " 'juicy',\n",
       " 'sausage',\n",
       " 'forget',\n",
       " 'master',\n",
       " 'send',\n",
       " 'pantry',\n",
       " 'stop',\n",
       " 'eat',\n",
       " 'longer',\n",
       " 'hungry',\n",
       " 'cat',\n",
       " 'decide',\n",
       " 'might',\n",
       " 'well',\n",
       " 'leave',\n",
       " 'mouse',\n",
       " 'hunt',\n",
       " 'another',\n",
       " 'day',\n",
       " 'pad',\n",
       " 'forty',\n",
       " 'wink',\n",
       " 'elsewhere',\n",
       " 'soon',\n",
       " 'country',\n",
       " 'mouse',\n",
       " 'realize',\n",
       " 'danger',\n",
       " 'past',\n",
       " 'lose',\n",
       " 'second',\n",
       " 'hastily',\n",
       " 'shake',\n",
       " 'hand',\n",
       " 'friend',\n",
       " 'say',\n",
       " 'thank',\n",
       " 'much',\n",
       " 'everything',\n",
       " 'must',\n",
       " 'rush',\n",
       " 'stand',\n",
       " 'shock',\n",
       " 'far',\n",
       " 'rather',\n",
       " 'sit',\n",
       " 'meal',\n",
       " 'acorns',\n",
       " 'peace',\n",
       " 'country',\n",
       " 'face',\n",
       " 'great',\n",
       " 'spread',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'surround',\n",
       " 'dangers',\n",
       " 'side',\n",
       " 'heart',\n",
       " 'mouth']"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['cmoutmou.txt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ySq0jJn8oexi"
   },
   "outputs": [],
   "source": [
    "# with open('DocTerms.pickle', 'wb') as handle:\n",
    "    # pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# with open('/content/DocTerms.pickle', 'rb') as handle:\n",
    "    # docT = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6d2zWznup1d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
